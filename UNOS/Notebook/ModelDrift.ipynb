{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52662a05-2966-4d92-8e51-db414019d162",
   "metadata": {},
   "source": [
    "#### Kullback-Leibler (KL) Divergence\n",
    "- KL divergence measures the relative entropy between two probability distributions. It's often used in machine learning and information theory.\n",
    "- Pros:\n",
    "    - Computationally efficient\n",
    "    - Directly interpretable in terms of information theory\n",
    "    - Works well for comparing discrete distributions\n",
    "- Cons:\n",
    "    - Not symmetric (KL(P||Q) ≠ KL(Q||P))\n",
    "    - Undefined if Q(x) = 0 and P(x) ≠ 0\n",
    "    - Doesn't satisfy `triangle inequality`\n",
    "        - The `triangle inequality` is a fundamental concept in mathematics, especially in geometry and linear algebra. It states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the third side. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897a022-e8f0-46cc-8f9b-edfbfb307f2e",
   "metadata": {},
   "source": [
    "#### Wasserstein Distance\n",
    "- Also known as Earth Mover's Distance, it measures the minimum \"cost\" of transforming one distribution into another.\n",
    "- Pros:\n",
    "    - Provides a true metric (symmetric and satisfies triangle inequality)\n",
    "    - Works well for continuous distributions\n",
    "    - More robust when distributions have little or no overlap\n",
    "- Cons:\n",
    "    - Can be computationally expensive, especially for high-dimensional data\n",
    "    - May be less intuitive to interpret than KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606b445-58f4-4a73-bfff-e133820d2ec3",
   "metadata": {},
   "source": [
    "#### Choosing Between Them\n",
    "- If your distributions are discrete and you're interested in information-theoretic interpretations, KL divergence might be preferable.\n",
    "    - A discrete distribution describes the probability distribution of a random variable that can take on a finite or countable number of distinct values. In simpler terms, it represents the likelihood of different outcomes in scenarios where the outcomes are distinct and separate (like rolling a die or flipping a coin). \n",
    "- If you're dealing with continuous distributions or need a true metric, Wasserstein distance could be more appropriate.\n",
    "    - A continuous distribution describes the probability distribution of a random variable that can take on any value within a given range or interval, which may be finite or infinite. Unlike discrete distributions, where probabilities are assigned to individual outcomes, continuous distributions involve probabilities over intervals, as the probability of any specific point is zero. \n",
    "- Consider computational resources: KL divergence is generally faster to compute.\n",
    "- Think about the nature of your data: Wasserstein distance is often better for comparing distributions with different supports.\n",
    "- Remember, the choice between these methods can significantly impact your analysis, so choose wisely based on your specific use case and data characteristics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a3531c-d536-453a-9b7a-6b4024d8f0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RFSurvival",
   "language": "python",
   "name": "survival_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
